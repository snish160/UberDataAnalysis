# -*- coding: utf-8 -*-
"""UberDataAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vtozIFgp9nxnvWLgkm1g3QNUDlOeeRC7
"""

import requests
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

def get_uber_data(api_key):
    url = "https://api.uber.com/v1/requests"
    headers = {"Authorization": "Bearer " + api_key}
    response = requests.get(url, headers=headers)
    data = response.json()
    return data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('uberdata.csv')

df.head() # to see the first few rows of the dataset
df.info() # to get the summary of the dataset
df.describe() # to get the statistical summary of the dataset

df = pd.read_csv('uberd.csv')

print(df.head())

# Check for missing values
print(df.isnull().sum())

# Remove any rows with missing values
df.dropna(inplace=True)

print(df.columns)

df['START_DATE*'] = pd.to_datetime(df['START_DATE*'])
df['END_DATE*'] = pd.to_datetime(df['END_DATE*'])

# Create a new column for trip duration
df['Duration'] = (df['END_DATE*'] - df['START_DATE*']).dt.seconds/3600

# Remove any rows with negative duration
df = df[df['Duration'] > 0]

# Convert Miles Driven column to numeric format
df['MILES*'] = pd.to_numeric(df['MILES*'], errors='coerce')

# Remove any rows with missing or invalid miles driven values
df = df[~df['MILES*'].isnull()]

# View the cleaned dataset
print(df.head())

plt.figure(figsize=(8, 6))
sns.countplot(x='PURPOSE*', data=df)
plt.title('Distribution of Trip Purposes')
plt.xlabel('PURPOSE*')
plt.ylabel('Count')
plt.show()

"""MAXIMUM COUNT = 175 for the PURPOSE = MEETINGS

"""

plt.figure(figsize=(8, 6))
sns.barplot(x='PURPOSE*', y='Duration', data=df, estimator=np.mean)
plt.title('Average Trip Duration by Purpose')
plt.xlabel('PURPOSE*')
plt.ylabel('Average Duration (hours)')
plt.show()

"""HIGHEST DURATION APPROX 3 HOURS FOR COMMUTE ."""

df['Month'] = df['START_DATE*'].dt.month
pivot_table = df.pivot_table(index='PURPOSE*', columns='Month', values='MILES*', aggfunc=np.sum)
plt.figure(figsize=(8, 6))
sns.heatmap(pivot_table, cmap='Blues')
plt.title('Miles Driven by Purpose and Month')
plt.xlabel('Month')
plt.ylabel('PURPOSE*')
plt.show()

"""MAXIMUM COUNTS IN THE HEATMAP is customer visits and meeting (BUISNESS) in the month of MARCH followed by APRIL."""

df['Hour'] = df['START_DATE*'].dt.hour
df['Day of Week'] = df['START_DATE*'].dt.day_name()

# Group trips by hour and day of the week and calculate the count
hourly_count = df.groupby(['Hour', 'Day of Week'])['START_DATE*'].count().reset_index()
hourly_count = hourly_count.pivot('Hour', 'Day of Week', 'START_DATE*')

# Plot heatmap of hourly count by day of the week
plt.figure(figsize=(12, 8))
sns.heatmap(hourly_count, cmap='YlGnBu')
plt.title('Hourly Count by Day of the Week')
plt.xlabel('Day of Week')
plt.ylabel('Hour')
plt.show()

df = pd.read_csv('uberd.csv')
df.columns

"""The busiest days are often Fridays as it involves both work and personal commutes , and the buiseist hours are evening and morning ."""

from sklearn.metrics import mean_squared_error
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

df['START_DATE*'] = pd.to_datetime(df['START_DATE*'], format='%m/%d/%Y %H:%M')
df['END_DATE*'] = pd.to_datetime(df['END_DATE*'], format='%m/%d/%Y %H:%M')
df['MILES*'] = df['MILES*'].astype(str).str.replace(',', '').astype(float)
df['PURPOSE*'] = df['PURPOSE*'].replace({'Meeting - Inside City': 'Meeting', 'Meeting- Outside City': 'Meeting'})

# Print the cleaned data
print(df.head())

# Create a new column 'Hour'
df['Hour'] = df['START_DATE*'].dt.hour

# Group the data by 'Hour' and 'Day of Week'
hourly_count = df.groupby(['Hour', 'Day of Week'])['START_DATE*'].count()

# Pivot the data to create a heatmap
hourly_count_pivot = hourly_count.reset_index().pivot('Hour', 'Day of Week', 'START_DATE*')

# Plot heatmap of hourly count by day of the week
plt.figure(figsize=(12, 8))
sns.heatmap(hourly_count_pivot, cmap='YlGnBu')
plt.title('Hourly Count by Day of the Week')
plt.xlabel('Day of Week')
plt.ylabel('Hour of Day')
plt.show()

print(df[['START_DATE*', 'Hour', 'Day of Week']].duplicated().sum())

print(df[['START_DATE*', 'Hour', 'Day of Week']].isnull().sum())

print(df[['START_DATE*', 'Hour', 'Day of Week']].dtypes)

df['Duration'] = (df['END_DATE*'] - df['START_DATE*']).dt.total_seconds() / 60

# Plot histogram of trip duration
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='Duration', kde=True)
plt.title('Distribution of Trip Duration')
plt.xlabel('Duration (minutes)')
plt.ylabel('Count')
plt.show()

# Group trips by purpose and calculate the count, miles driven, and average miles per trip
user_data = df.groupby('PURPOSE*')['START_DATE*'].agg(['count'])
user_data['MILES*'] = df.groupby('PURPOSE*')['MILES*'].sum()
user_data['Average Miles per Trip'] = user_data['MILES*'] / user_data['count']

# Plot bar chart of average miles per trip by purpose
plt.figure(figsize=(10, 6))
sns.barplot(x=user_data.index, y='Average Miles per Trip', data=user_data)
plt.title('Average Miles per Trip by Purpose')
plt.xlabel('Purpose')
plt.ylabel('Average Miles per Trip')
plt.show()

# Load the dataset

# Data cleaning
# (Same code as before)
### K MEANS CLUSTERING
# Extract relevant features for clustering
X = df[['MILES*', 'Fare Amount']]

from sklearn.preprocessing import StandardScaler

# Standardize the features
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# Cluster the data using KMeans
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X_std)

# Plot the clusters
plt.scatter(X_std[:, 0], X_std[:, 1], c=kmeans.labels_, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=300, color='black')
plt.xlabel('Miles Driven (Standardized)')
plt.ylabel('Fare Amount (Standardized)')
plt.title('KMeans Clustering')
plt.show()

# Import necessary libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# DECISION TREE

# Separate the features and target variable
X = df[['MILES*', 'PURPOSE*']]
y = df['CATEGORY*']

# Convert categorical features into numerical using One-Hot Encoding
X = pd.get_dummies(X)

# Split the dataset into train and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a decision tree classifier
tree = DecisionTreeClassifier()

# Train the model using the training set
tree.fit(X_train, y_train)

# Predict on the test set
y_pred = tree.predict(X_test)

# Calculate accuracy score and confusion matrix
acc = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Accuracy:", acc)
print("Confusion Matrix:", cm)

"""The purpose of this code is to predict the category of Uber rides (Business or Personal) based on the miles driven and the purpose of the drive.
The accuracy score tells us how well the model is performing, and the confusion matrix shows us the number of true positives, true negatives, false positives, and false negatives. These metrics help us to understand the performance of the model and identify areas for improvement.
"""

# Load the dataset

### RANDOM FOREST REGRESSION TO PREDICT RIDER DEMAND
# Data cleaning
# (Same code as before)

# Extract date and hour from Start Date column
df['Date'] = df['START_DATE*'].dt.date
df['Hour'] = df['START_DATE*'].dt.hour

# Group trips by date, hour, and location and calculate the count
location_count = df.groupby(['Date', 'Hour', 'START*'])['START_DATE*'].count().reset_index()

# Feature engineering
location_count['Day of Week'] = pd.to_datetime(location_count['Date']).dt.day_name()
location_count = location_count.drop(['Date'], axis=1)

# One-hot encoding
location_count = pd.get_dummies(location_count, columns=['Day of Week', 'START*'])

# Train-test split
X = location_count.drop(['START_DATE*'], axis=1)
y = location_count['START_DATE*']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model training and prediction
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import mean_squared_error, r2_score

# Model evaluation
print('Mean Squared Error:', mean_squared_error(y_test, y_pred))
# print('R-squared:', r2_score(y_test, y_pred))

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Print the predicted rider demand for the testing set
print(y_pred)

df.columns

print(df.head())